\documentclass[a4paper, 10pt]{article}

\usepackage[resetfonts]{cmap}
\usepackage[english, czech]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{multicol}

\usepackage{hyperref}
\hypersetup{unicode=true}
\usepackage{microtype}

\title{WalkAuth\\
\large Závěrečná zpráva ze studentského projektu do předmětu PV021
}


\author{Čechák, Jaroslav\\
	\texttt{410322@mail.muni.cz}
	\and
	Effenberger, Tomáš\\
	\texttt{410350@mail.muni.cz}
	\and
	Mauritz, Jiří\\
	\texttt{409972@mail.muni.cz}
}
\date{}

\begin{document}
\maketitle


\begin{abstract}
  Cílem tohoto projektu bylo vytvořit neuronovou síť schopnou porozumnět údajům z akcelerometru naměřeným během klasické chůze několika lidí. Naučili jsme neuronovou síť rozpoznávat jednoho z nich a na dalších vzorcích jsme zjišťovali, zda je možné ho odlišit od ostatních jen podle stylu chůze. Strukturu vstupních dat, neuronovou síť a proces učení jsme navrhli co nejvíce obecné a konfigurovatelné, abychom mohli pozorovat důsledky změny parametrů na výslednou úspěšnost. Použili jsme klasickou vícevrstvou neuronovou síť a učení s učitelem aplikujeme pomocí gradientního sestupu a zpětné propagace.
\end{abstract}

\section{Zpracování dat}
      \begin{multicols}{2}
    \subsection{Formát dat}
        Data byla stažena ze zdrojů online repozitáře UCI Machine Learning Repository \footnote{\url{http://archive.ics.uci.edu/ml/datasets/User+Identification+From+Walking+Activity}}. Dataset se jmenuje \textit{User Identification From Walking Activity Data Set}  a je strukturován do 22 csv souborů, kde každý soubor představuje chůzi jednoho uživatele. Každý záznam z akcelerometru obsahuje čas měření a derivaci pohybu v každém ze tří směrů x,y,z. Časy měření nejsou synchornizované, ale jsou závislé na rycholsti zápisu akcelerometru. Úlohu jsme si zjednodušili tím, že jsme ignorovali čas měření, protože všichni používali stejný akcelerometr, a tedy stejný počet záznamů odpovídal přibližně stejnému počtu milisekund v každém záznamu. Čas chůze je různý pro jednotlivé uživatele a pohybuje se od 30 sekund do 11 minut.
    \subsection{Rozdělení dat}
        Jeden vzorek (v dokumentaci označovaný jako sample) odpovídá určitému počtu záznamů, který lze určit parametrem. Výchozí hodnota je 100 záznamů, což odpovídá přibližně třem sekundám chůze. Takový vzor následně slouží jako jeden vstup do neuronové sítě. Protože je jeden záznam složen ze tří číselných hodnot, vstup do neuronové sítě je třikrát větší, než je počet záznamů v jednom vzorku.
        Rozhodli jsme se rozdělit data na tři množiny: trénovací, testovací a validační. Rozdělení dat mezi tyto množiny je možné upravit pomocí parametrů, ale výchozí hodnoty jsou trénovací 70\%, testovací 20\% a validační 10\%. Trénovací data používáme pro učení sítě, testovací použijeme na závěr učení pro otestování úspěšnosti naučené sítě a validační množina slouží ke kontrole přeučení. Rozdělení vzorků do těchto tří množin je v každém procesu učení náhodné. Výskyt pozitivních a negativních vzorků je rovnoměrný pro všechny tři množiny.

    \subsection{Normalizace dat}
        Všechny tři množiny jsme normalizovali podle střední hodnoty a směrodatné odchylky naměřené v trénovacích datech. Takto jsme nezavedli žádnou informaci o testovacích datech do trénovací množiny. Normalizovali jsme data na střední hodnotu 0 a směrodatnou odchylku 1. Původní data mají střední hodnotu $2,26$ a směrodatnou odchylku $5,57$.
      \end{multicols}
\section{Popis implementace}
  \begin{multicols}{2}
    Projekt jsme implementovali v jazyce Java.
    Zpracování dat pro neuronovou síť zajišťuje třída \texttt{DataManager} (přičemž normalizaci deleguje na třídu \texttt{Normalization} a jeden vzorek je reprezentován třídou \texttt{Sample}).
    Pro reprezentaci neuronové sítě (její topologie a vah) slouží třída \texttt{NeuralNetwork}. Tato třída umožňuje také provádět dopředný výpočet pro zadaný vstupní vzorek.
    Učení neuronové sítě pak zajišťuje třída \texttt{NeuralNetoworkLearning}. Proces učení začíná nastavením náhodných počátečních vah (popsané podrobněji níže), po kterém následuje klasický gradientní sestup využívající zpětné propagace pro výpočet gradientu čtvercové chyby vzhledem k hodnotám vah.
    Třída \texttt{Evaluation} implementuje všechny použité evaluační metriky (čtvercová chyba, RMSE, precision, recall a F1).
    Dále už projekt obsahuje pouze několik pomocných tříd, např. pro práci s maticemi, výpočet aktivační funkce (hyperbolický tangens) a její derivace a logování.
    Většina funkcionality našeho projektu je pokrytá jednotkovými testy.
    V následujících odstavcích popisujeme některé zajímavé detaily.

    \subsection{Inicializace vah}
      Počáteční váhy jsme nastavili náhodně v intervalu $[-w,w]$, kde $w$ je hodnota společná pro váhy vstupující do stejné vrstvy v neuronové síti. Hodnotu $w$ jsme spočítali takto $w = \sqrt{\frac{3}{d}}$ kde $d$ je počet vstupů do jednoho neuronu vyšší vrstvy. Takto váhy do neuronu s vyšším počtem vstupů byly menší než váhy do neuronu s méně vstupy, a proto jejich potenciál byl srovnatelný na počátku učení. Vycházeli jsme z doporučení v materiálech předmětu PV021.

    \subsection{Test zpětné propagace}
    Jedním z klíčových algoritmů našeho projektu je zpětná propagace.
    Rozhodli jsme se ho pořádně otestovat jednotkovými testy, ručně však bylo možné připravit pouze extrémně triviální případy.
    Proto jsme implementovaly jednoduchý numerický výpočet zpětné propagace a pak napsali řadu testů, které kontrolují, že pro různé topologie, aktuální nastavení vah a trénovací data vracejí numerický výpočet gradientu a zpětná propagace stejné výsledky (až na malou odchylku).
    Numerický výpočet gradientu funguje následovně: pro každou váhu $i$ spočítáme pro dostatečně malé $\delta$ gradient vzhledem k této váze zvlášť numericky pomocí vzorce
    $$\frac{E(\ldots, \theta_i + \delta, \ldots) - E(\ldots, \theta_i - \delta, \ldots)}{2\delta}$$
    kde $E$ je čtvercová chyba pro dané nastavení vah.
    Takový výpočet gradientu je sice neefektivní (takže ho nelze použít při učení), zato však velmi jednoduchý na implementaci a mnohem méně náchylný na chyby než algoritmus zpětné propagace.
    Pořádné otestování zpětné propagace se vyplatilo, protože v prvotní implementaci bylo hned několik chyb, které jsme díky testům okamžitě odhalili a mohli odladit ještě před zapojením funkce do gradientního sestupu.


  \end{multicols}
\section{Naměřené výsledky a použité parametry}
  \begin{multicols}{2}
  TODO: Co jsme neuronovou síť naučili, jak dobře, jak dlouho jí to trvalo s jakýma ,,magic`` konstantama, ...
  \end{multicols}
\section{Diskuse}
  \begin{multicols}{2}
    TODO: Proč byla síť neúspěšná, zmínit, že vzorky mohli být z jiných momentů chůze
  \end{multicols}
\appendix
\section{Návod na použití}
  TODO: Co je potřeba pro spuštění našeho programu, jak spustit náš program, jak předat parametry, kde jsou zdrojáky, ...
\section{Rozdělení práce}



\end{document}
